{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"paul.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "# MODEL = \"llama3.1\"\n",
    "\n",
    "# We'll be using Gemma 2B for this example.\n",
    "MODEL = \"gemma:2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "Let's start by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 9\n",
      "Length of a page: 3272\n",
      "Content of a page: 10% a week. And while 110 may not seem much better\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 23\n",
      "Length of a chunk: 1236\n",
      "Content of a chunk: took better advantage of it than Stripe. At YC we use the term\n",
      "\"Collison installation\" for the technique they invented. More\n",
      "diffident founders ask \"Will you try our beta?\" and if the answer is\n",
      "yes, they say \"Great, we'll send you a link.\" But the Collison\n",
      "brothers weren't going to wait. When anyone agreed to try Stripe\n",
      "they'd say \"Right then, give me your laptop\" and set them up on\n",
      "the spot.\n",
      "There are two reasons founders resist going out and recruiting\n",
      "users individually. One is a combination of shyness and laziness.\n",
      "They'd rather sit at home writing code than go out and talk to a\n",
      "bunch of strangers and probably be rejected by most of them.\n",
      "But for a startup to succeed, at least one founder (usually the\n",
      "CEO) will have to spend a lot of time on sales and marketing. [2]\n",
      "The other reason founders ignore this path is that the absolute\n",
      "numbers seem so small at first. This can't be how the big, famous\n",
      "startups got started, they think. The mistake they make is to\n",
      "underestimate the power of compound growth. We encourage\n",
      "every startup to measure their progress by weekly growth rate. If\n",
      "you have 100 users, you need to get 10 more next week to grow\n",
      "8/6/24, 11:04 AM Do Things that Don't Scale\n",
      "https://paulgraham.com/ds.html 1/9\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "chunks = splitter.split_documents(pages)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "\n",
    "<img src='images/vectorstore.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='afbe345d-ad11-42f2-864b-6342602ca448', metadata={'source': 'paul.pdf', 'page': 3}, page_content='Experience\\nI was trying to think of a phrase to convey how extreme your\\nattention to users should be, and I realized Steve Jobs had\\nalready done it: insanely great. Steve wasn\\'t just using \"insanely\"\\nas a synonym for \"very.\" He meant it more literally — that one\\nshould focus on quality of execution to a degree that in everyday\\nlife would be considered pathological.\\nAll the most successful startups we\\'ve funded have, and that\\nprobably doesn\\'t surprise would-be founders. What novice\\nfounders don\\'t get is what insanely great translates to in a larval\\nstartup. When Steve Jobs started using that phrase, Apple was\\nalready an established company. He meant the Mac (and its\\ndocumentation and even packaging — such is the nature of\\nobsession) should be insanely well designed and manufactured.\\nThat\\'s not hard for engineers to grasp. It\\'s just a more extreme\\nversion of designing a robust and elegant product.\\nWhat founders have a hard time grasping (and Steve himself\\nmight have had a hard time grasping) is what insanely great\\nmorphs into as you roll the time slider back to the first couple\\nmonths of a startup\\'s life. It\\'s not the product that should be\\ninsanely great, but the experience of being your user. The product\\nis just one component of that. For a big company it\\'s necessarily\\nthe dominant one. But you can and should give users an insanely\\ngreat experience with an early, incomplete, buggy product, if you\\nmake up the difference with attentiveness.'),\n",
       " Document(id='e8815ba0-603d-4700-addd-9fec2b79a32b', metadata={'source': 'paul.pdf', 'page': 2}, page_content=\"in charge of their narrow domain of building things, rather than\\nrunning the whole show. You can be ornery when you're Scotty,\\nbut not when you're Kirk.\\nAnother reason founders don't focus enough on individual\\ncustomers is that they worry it won't scale. But when founders of\\nlarval startups worry about this, I point out that in their current\\nstate they have nothing to lose. Maybe if they go out of their way\\nto make existing users super happy, they'll one day have too\\nmany to do so much for. That would be a great problem to have.\\nSee if you can make it happen. And incidentally, when it does,\\nyou'll find that delighting customers scales better than you\\nexpected. Partly because you can usually find ways to make\\nanything scale more than you would have predicted, and partly\\nbecause delighting customers will by then have permeated your\\nculture.\\nI have never once seen a startup lured down a blind alley by\\ntrying too hard to make their initial users happy.\\nBut perhaps the biggest thing preventing founders from realizing\\nhow attentive they could be to their users is that they've never\\nexperienced such attention themselves. Their standards for\\ncustomer service have been set by the companies they've been\\ncustomers of, which are mostly big ones. Tim Cook doesn't send\\nyou a hand-written note after you buy a laptop. He can't. But you\\ncan. That's one advantage of being small: you can provide a level\\nof service no big company can. [6]\"),\n",
       " Document(id='4d06218c-5ee4-45ea-866e-5b39ee1b744e', metadata={'source': 'paul.pdf', 'page': 1}, page_content='larval startups by the standards of established ones. They\\'re like\\nsomeone looking at a newborn baby and concluding \"there\\'s no\\nway this tiny creature could ever accomplish anything.\"\\nIt\\'s harmless if reporters and know-it-alls dismiss your startup.\\nThey always get things wrong. It\\'s even ok if investors dismiss\\nyour startup; they\\'ll change their minds when they see growth.\\nThe big danger is that you\\'ll dismiss your startup yourself. I\\'ve\\nseen it happen. I often have to encourage founders who don\\'t see\\nthe full potential of what they\\'re building. Even Bill Gates made\\nthat mistake. He returned to Harvard for the fall semester after\\nstarting Microsoft. He didn\\'t stay long, but he wouldn\\'t have\\nreturned at all if he\\'d realized Microsoft was going to be even a\\nfraction of the size it turned out to be. [4]\\nThe question to ask about an early stage startup is not \"is this\\ncompany taking over the world?\" but \"how big could this company\\nget if the founders did the right things?\" And the right things\\noften seem both laborious and inconsequential at the time.\\nMicrosoft can\\'t have seemed very impressive when it was just a\\ncouple guys in Albuquerque writing Basic interpreters for a\\nmarket of a few thousand hobbyists (as they were then called),\\nbut in retrospect that was the optimal path to dominating\\nmicrocomputer software. And I know Brian Chesky and Joe\\nGebbia didn\\'t feel like they were en route to the big time as they\\nwere taking \"professional\" photos of their first hosts\\' apartments.'),\n",
       " Document(id='0b26df95-b5d3-4b9d-a6ca-a0d5f9b077b4', metadata={'source': 'paul.pdf', 'page': 4}, page_content='them. It was one of many unforeseen advantages of the YC\\nmodel (and specifically of making YC big) that B2B startups now\\nhave an instant market of hundreds of other startups ready at\\nhand.\\nMeraki\\nFor hardware startups there\\'s a variant of doing things that don\\'t\\nscale that we call \"pulling a Meraki.\" Although we didn\\'t fund\\nMeraki, the founders were Robert Morris\\'s grad students, so we\\nknow their history. They got started by doing something that\\nreally doesn\\'t scale: assembling their routers themselves.\\nHardware startups face an obstacle that software startups don\\'t.\\nThe minimum order for a factory production run is usually several\\nhundred thousand dollars. Which can put you in a catch-22:\\nwithout a product you can\\'t generate the growth you need to\\nraise the money to manufacture your product. Back when\\nhardware startups had to rely on investors for money, you had to\\nbe pretty convincing to overcome this. The arrival of\\ncrowdfunding (or more precisely, preorders) has helped a lot. But\\neven so I\\'d advise startups to pull a Meraki initially if they can.\\nThat\\'s what Pebble did. The Pebbles assembled the first several\\nhundred watches themselves. If they hadn\\'t gone through that\\nphase, they probably wouldn\\'t have sold $10 million worth of\\nwatches when they did go on Kickstarter.\\nLike paying excessive attention to early customers, fabricating\\nthings yourself turns out to be valuable for hardware startups.\\nYou can tweak the design faster when you\\'re the factory, and you')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"What can you get away with when you only have a small number of users?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I do not have access to real-time information and cannot provide the current president of the United States. For the most up-to-date information, please check a reputable news source or government website.', additional_kwargs={}, response_metadata={'model': 'gemma:2b', 'created_at': '2024-12-26T06:16:44.33918308Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3546985669, 'load_duration': 23763432, 'prompt_eval_count': 31, 'prompt_eval_duration': 641000000, 'eval_count': 42, 'eval_duration': 2880000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-44b31388-a7f9-4f95-aa12-e3deb78f4b8a-0', usage_metadata={'input_tokens': 31, 'output_tokens': 42, 'total_tokens': 73})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to real-time information and cannot provide the current president of the United States. For the most up-to-date information, please check a reputable news source or government website.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "print(chain.invoke(\"Who is the president of the United States?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Anna's sister is Susan.\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What can you get away with when you only have a small number of users?\n",
      "Answer: According to the context, you can fabricate things yourself to turn your product into a hit. This is especially helpful for hardware startups who have to deal with minimum order requirements from investors.\n",
      "*************************\n",
      "\n",
      "Question: What's the most common unscalable thing founders have to do at the start?\n",
      "Answer: According to the context, the most common unscalable thing founders have to do at the start is assembling their product themselves.\n",
      "*************************\n",
      "\n",
      "Question: What's one of the biggest things inexperienced founders and investors get wrong about startups?\n",
      "Answer: One of the biggest things inexperienced founders and investors get wrong about startups is that they're not as important as they think they are. They often dismiss startups as a necessary evil, but they can actually have a significant impact on the world.\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What can you get away with when you only have a small number of users?\",\n",
    "    \"What's the most common unscalable thing founders have to do at the start?\",\n",
    "    \"What's one of the biggest things inexperienced founders and investors get wrong about startups?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mle-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
